{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning and Preprocessing\n",
        "Clean the dataset by handling missing values, removing duplicates, correcting data types, and standardizing formats. This step ensures the data is ready for analysis and modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Set display options for better readability\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset...\n",
            "Dataset shape: (196553, 52)\n",
            "Number of companies: 196553\n",
            "Number of features: 52\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset with proper data types\n",
        "print(\"Loading dataset...\")\n",
        "df = pd.read_csv('companies_with_success_labels.csv', \n",
        "                parse_dates=['founded_at', 'closed_at', 'first_investment_at', \n",
        "                            'last_investment_at', 'first_funding_at', 'last_funding_at',\n",
        "                            'first_milestone_at', 'last_milestone_at', 'created_at', 'updated_at'],\n",
        "                low_memory=False)\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Number of companies: {df.shape[0]}\")\n",
        "print(f\"Number of features: {df.shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checking for duplicates...\n",
            "Number of duplicate rows: 0\n",
            "Number of duplicate IDs: 0\n"
          ]
        }
      ],
      "source": [
        "# Check for duplicate rows\n",
        "print(\"\\nChecking for duplicates...\")\n",
        "duplicate_rows = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
        "\n",
        "# Check for duplicate company identifiers\n",
        "duplicate_ids = df['id'].duplicated().sum()\n",
        "print(f\"Number of duplicate IDs: {duplicate_ids}\")\n",
        "\n",
        "# Remove duplicate rows if any\n",
        "if duplicate_rows > 0:\n",
        "    df = df.drop_duplicates()\n",
        "    print(f\"Removed {duplicate_rows} duplicate rows. New shape: {df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checking missing values...\n",
            "                     Missing Values  Missing Percentage\n",
            "parent_id                    196553              100.00\n",
            "ROI                          195827               99.63\n",
            "first_investment_at          193970               98.69\n",
            "last_investment_at           193970               98.69\n",
            "invested_companies           193962               98.68\n",
            "investment_rounds            193962               98.68\n",
            "closed_at                    193933               98.67\n",
            "short_description            189422               96.37\n",
            "funding_total_usd            168679               85.82\n",
            "last_funding_at              165046               83.97\n",
            "first_funding_at             165046               83.97\n",
            "funding_rounds               164846               83.87\n",
            "state_code                   145650               74.10\n",
            "twitter_username             115962               59.00\n",
            "tag_list                     115101               58.56\n",
            "lat                          112701               57.34\n",
            "lng                          112701               57.34\n",
            "city                         112663               57.32\n",
            "country_code                 108563               55.23\n",
            "founded_at                   105326               53.59\n"
          ]
        }
      ],
      "source": [
        "# Check missing values\n",
        "print(\"\\nChecking missing values...\")\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
        "missing_info = pd.DataFrame({\n",
        "    'Missing Values': missing_values,\n",
        "    'Missing Percentage': missing_percentage\n",
        "})\n",
        "print(missing_info.sort_values('Missing Percentage', ascending=False).head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Identifying columns to drop...\n",
            "Columns with >95% missing values: ['parent_id', 'closed_at', 'short_description', 'first_investment_at', 'last_investment_at', 'investment_rounds', 'invested_companies', 'ROI']\n",
            "Redundant columns: ['Unnamed: 0.1']\n",
            "Columns with limited analytical value: ['logo_url', 'logo_width', 'logo_height', 'parent_id']\n",
            "\n",
            "Total columns to drop: 13\n",
            "['parent_id', 'closed_at', 'short_description', 'first_investment_at', 'last_investment_at', 'investment_rounds', 'invested_companies', 'ROI', 'Unnamed: 0.1', 'logo_url', 'logo_width', 'logo_height', 'parent_id']\n",
            "\n",
            "Shape after dropping columns: (196553, 40)\n"
          ]
        }
      ],
      "source": [
        "# Identify columns to drop based on high missing percentage or redundancy\n",
        "print(\"\\nIdentifying columns to drop...\")\n",
        "\n",
        "# Columns with very high missing values (>95%) that aren't critical\n",
        "high_missing_cols = missing_info[missing_info['Missing Percentage'] > 95].index.tolist()\n",
        "print(f\"Columns with >95% missing values: {high_missing_cols}\")\n",
        "\n",
        "# Redundant identifier columns\n",
        "redundant_cols = ['Unnamed: 0.1']  # This appears to be just a row index\n",
        "print(f\"Redundant columns: {redundant_cols}\")\n",
        "\n",
        "# Columns with limited analytical value\n",
        "limited_value_cols = ['logo_url', 'logo_width', 'logo_height', 'parent_id']\n",
        "print(f\"Columns with limited analytical value: {limited_value_cols}\")\n",
        "\n",
        "# Combine all columns to drop\n",
        "columns_to_drop = high_missing_cols + redundant_cols + limited_value_cols\n",
        "print(f\"\\nTotal columns to drop: {len(columns_to_drop)}\")\n",
        "print(columns_to_drop)\n",
        "\n",
        "# Drop the identified columns\n",
        "df_cleaned = df.drop(columns=columns_to_drop, errors='ignore')\n",
        "print(f\"\\nShape after dropping columns: {df_cleaned.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Handling missing values for important columns...\n",
            "Filled 73367 missing values in category_code with 'unknown'\n",
            "Filled 108563 missing values in country_code with 'unknown'\n",
            "Filled 145650 missing values in state_code with 'unknown'\n",
            "Filled 112663 missing values in city with 'unknown'\n",
            "Filled 0 missing values in region with 'unknown'\n",
            "Filled 23 missing values in name with empty string\n",
            "Filled 26 missing values in normalized_name with empty string\n",
            "Filled 70008 missing values in domain with empty string\n",
            "Filled 70008 missing values in homepage_url with empty string\n",
            "Filled 115962 missing values in twitter_username with empty string\n",
            "Filled 104505 missing values in description with empty string\n",
            "Filled 69582 missing values in overview with empty string\n",
            "Filled 115101 missing values in tag_list with empty string\n",
            "Filled 164846 missing values in funding_rounds with 0\n",
            "Filled 168679 missing values in funding_total_usd with 0\n",
            "Filled 104854 missing values in milestones with 0\n",
            "Filled 66886 missing values in relationships with 0\n",
            "Keeping 105326 missing values in founded_at as NaN\n",
            "Keeping 165046 missing values in first_funding_at as NaN\n",
            "Keeping 165046 missing values in last_funding_at as NaN\n",
            "Keeping 104854 missing values in first_milestone_at as NaN\n",
            "Keeping 104854 missing values in last_milestone_at as NaN\n"
          ]
        }
      ],
      "source": [
        "# Handle missing values for important columns\n",
        "print(\"\\nHandling missing values for important columns...\")\n",
        "\n",
        "# For categorical columns, fill with 'unknown'\n",
        "categorical_cols = ['category_code', 'country_code', 'state_code', 'city', 'region']\n",
        "for col in categorical_cols:\n",
        "    if col in df_cleaned.columns:\n",
        "        missing_before = df_cleaned[col].isnull().sum()\n",
        "        df_cleaned[col] = df_cleaned[col].fillna('unknown')\n",
        "        print(f\"Filled {missing_before} missing values in {col} with 'unknown'\")\n",
        "\n",
        "# For text columns, fill with empty string\n",
        "text_cols = ['name', 'normalized_name', 'domain', 'homepage_url', 'twitter_username', \n",
        "             'short_description', 'description', 'overview', 'tag_list']\n",
        "for col in text_cols:\n",
        "    if col in df_cleaned.columns:\n",
        "        missing_before = df_cleaned[col].isnull().sum()\n",
        "        df_cleaned[col] = df_cleaned[col].fillna('')\n",
        "        print(f\"Filled {missing_before} missing values in {col} with empty string\")\n",
        "\n",
        "# For numeric columns related to funding, fill with 0\n",
        "funding_cols = ['funding_rounds', 'funding_total_usd', 'investment_rounds', 'invested_companies', 'milestones', 'relationships']\n",
        "for col in funding_cols:\n",
        "    if col in df_cleaned.columns:\n",
        "        missing_before = df_cleaned[col].isnull().sum()\n",
        "        df_cleaned[col] = df_cleaned[col].fillna(0)\n",
        "        print(f\"Filled {missing_before} missing values in {col} with 0\")\n",
        "\n",
        "# For date columns, we'll leave as NaN as imputing dates could introduce bias\n",
        "date_cols = ['founded_at', 'closed_at', 'first_investment_at', 'last_investment_at', \n",
        "             'first_funding_at', 'last_funding_at', 'first_milestone_at', 'last_milestone_at']\n",
        "for col in date_cols:\n",
        "    if col in df_cleaned.columns:\n",
        "        print(f\"Keeping {df_cleaned[col].isnull().sum()} missing values in {col} as NaN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Standardizing text fields...\n",
            "Cleaned text in name\n",
            "Cleaned text in normalized_name\n",
            "Cleaned text in description\n",
            "Cleaned text in overview\n",
            "Standardized URLs in homepage_url\n",
            "Standardized URLs in domain\n"
          ]
        }
      ],
      "source": [
        "# Standardize text fields\n",
        "print(\"\\nStandardizing text fields...\")\n",
        "\n",
        "# Function to clean text fields\n",
        "def clean_text(text):\n",
        "    if pd.isna(text) or text == '':\n",
        "        return ''\n",
        "    # Convert to string in case it's not\n",
        "    text = str(text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Apply to relevant text columns\n",
        "text_cols_to_clean = ['name', 'normalized_name', 'short_description', 'description', 'overview']\n",
        "for col in text_cols_to_clean:\n",
        "    if col in df_cleaned.columns:\n",
        "        df_cleaned[col] = df_cleaned[col].apply(clean_text)\n",
        "        print(f\"Cleaned text in {col}\")\n",
        "\n",
        "# Standardize URLs\n",
        "url_cols = ['homepage_url', 'domain']\n",
        "for col in url_cols:\n",
        "    if col in df_cleaned.columns:\n",
        "        # Convert to lowercase\n",
        "        df_cleaned[col] = df_cleaned[col].str.lower()\n",
        "        print(f\"Standardized URLs in {col}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Standardizing geographic data...\n",
            "Converted country codes to uppercase\n",
            "Converted state codes to uppercase\n",
            "Found 0 invalid latitude values\n",
            "Found 0 invalid longitude values\n",
            "Set invalid coordinates to NaN\n"
          ]
        }
      ],
      "source": [
        "# Standardize and validate geographic data\n",
        "print(\"\\nStandardizing geographic data...\")\n",
        "\n",
        "# Convert country codes to uppercase\n",
        "if 'country_code' in df_cleaned.columns:\n",
        "    df_cleaned['country_code'] = df_cleaned['country_code'].str.upper()\n",
        "    print(\"Converted country codes to uppercase\")\n",
        "\n",
        "# Convert state codes to uppercase\n",
        "if 'state_code' in df_cleaned.columns:\n",
        "    df_cleaned['state_code'] = df_cleaned['state_code'].str.upper()\n",
        "    print(\"Converted state codes to uppercase\")\n",
        "\n",
        "# Check for invalid latitude/longitude values\n",
        "if 'lat' in df_cleaned.columns and 'lng' in df_cleaned.columns:\n",
        "    # Valid latitude range: -90 to 90\n",
        "    invalid_lat = ((df_cleaned['lat'] < -90) | (df_cleaned['lat'] > 90)) & df_cleaned['lat'].notnull()\n",
        "    # Valid longitude range: -180 to 180\n",
        "    invalid_lng = ((df_cleaned['lng'] < -180) | (df_cleaned['lng'] > 180)) & df_cleaned['lng'].notnull()\n",
        "\n",
        "    print(f\"Found {invalid_lat.sum()} invalid latitude values\")\n",
        "    print(f\"Found {invalid_lng.sum()} invalid longitude values\")\n",
        "\n",
        "    # Set invalid coordinates to NaN\n",
        "    df_cleaned.loc[invalid_lat, 'lat'] = np.nan\n",
        "    df_cleaned.loc[invalid_lng, 'lng'] = np.nan\n",
        "    print(\"Set invalid coordinates to NaN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Correcting data types...\n",
            "Converted entity_id to numeric type\n",
            "Converted funding_rounds to numeric type\n",
            "Converted funding_total_usd to numeric type\n",
            "Converted milestones to numeric type\n",
            "Converted relationships to numeric type\n",
            "Converted success_status to numeric type\n",
            "Converted success_funding to numeric type\n",
            "Converted success_roi to numeric type\n",
            "Converted success_age to numeric type\n",
            "Converted success_score to numeric type\n",
            "Converted success_binary to numeric type\n",
            "Converted entity_type to category type\n",
            "Converted status to category type\n",
            "Converted category_code to category type\n",
            "Converted country_code to category type\n",
            "Converted state_code to category type\n",
            "Converted region to category type\n",
            "Converted success_class to category type\n"
          ]
        }
      ],
      "source": [
        "# Correct data types\n",
        "print(\"\\nCorrecting data types...\")\n",
        "\n",
        "# Ensure numeric columns are properly typed\n",
        "numeric_cols = ['entity_id', 'funding_rounds', 'funding_total_usd', 'investment_rounds', \n",
        "                'invested_companies', 'milestones', 'relationships', 'ROI', \n",
        "                'success_status', 'success_funding', 'success_roi', 'success_age', \n",
        "                'success_score', 'success_binary']\n",
        "\n",
        "for col in numeric_cols:\n",
        "    if col in df_cleaned.columns:\n",
        "        # Check if column contains any non-numeric values\n",
        "        non_numeric = pd.to_numeric(df_cleaned[col], errors='coerce').isnull() & df_cleaned[col].notnull()\n",
        "        if non_numeric.sum() > 0:\n",
        "            print(f\"Found {non_numeric.sum()} non-numeric values in {col}\")\n",
        "            # Convert to numeric, coercing errors to NaN\n",
        "            df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')\n",
        "        else:\n",
        "            # Convert to appropriate numeric type\n",
        "            if col in ['entity_id', 'funding_rounds', 'investment_rounds', \n",
        "                      'invested_companies', 'milestones', 'relationships',\n",
        "                      'success_status', 'success_funding', 'success_roi', \n",
        "                      'success_age', 'success_score', 'success_binary']:\n",
        "                df_cleaned[col] = df_cleaned[col].astype('Int64')  # nullable integer type\n",
        "            else:\n",
        "                df_cleaned[col] = pd.to_numeric(df_cleaned[col])\n",
        "        print(f\"Converted {col} to numeric type\")\n",
        "\n",
        "# Ensure categorical columns are properly typed\n",
        "categorical_cols = ['entity_type', 'status', 'category_code', 'country_code', \n",
        "                   'state_code', 'region', 'success_class']\n",
        "for col in categorical_cols:\n",
        "    if col in df_cleaned.columns:\n",
        "        df_cleaned[col] = df_cleaned[col].astype('category')\n",
        "        print(f\"Converted {col} to category type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Creating data quality flags...\n",
            "Companies with complete core information: 196553\n",
            "Companies with location information: 196553\n",
            "Companies with funding information: 196553\n",
            "Companies with founding date: 91227\n"
          ]
        }
      ],
      "source": [
        "# Create additional cleaning flags for tracking data quality\n",
        "print(\"\\nCreating data quality flags...\")\n",
        "\n",
        "# Flag for companies with complete core information\n",
        "core_cols = ['name', 'status', 'category_code']\n",
        "df_cleaned['has_complete_core_info'] = df_cleaned[core_cols].notnull().all(axis=1)\n",
        "print(f\"Companies with complete core information: {df_cleaned['has_complete_core_info'].sum()}\")\n",
        "\n",
        "# Flag for companies with location information\n",
        "location_cols = ['country_code', 'city']\n",
        "df_cleaned['has_location_info'] = df_cleaned[location_cols].notnull().all(axis=1)\n",
        "print(f\"Companies with location information: {df_cleaned['has_location_info'].sum()}\")\n",
        "\n",
        "# Flag for companies with funding information\n",
        "funding_cols = ['funding_rounds', 'funding_total_usd']\n",
        "df_cleaned['has_funding_info'] = df_cleaned[funding_cols].notnull().all(axis=1)\n",
        "print(f\"Companies with funding information: {df_cleaned['has_funding_info'].sum()}\")\n",
        "\n",
        "# Flag for companies with founding date\n",
        "df_cleaned['has_founding_date'] = df_cleaned['founded_at'].notnull()\n",
        "print(f\"Companies with founding date: {df_cleaned['has_founding_date'].sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checking for outliers in numeric columns...\n",
            "Found 27874 outliers in funding_total_usd\n",
            "Found 31707 outliers in funding_rounds\n",
            "Found 12366 outliers in relationships\n",
            "Found 6009 outliers in age_years\n",
            "Capping 197 extreme funding values at 223068000.00011572\n"
          ]
        }
      ],
      "source": [
        "# Check for outliers in numeric columns\n",
        "print(\"\\nChecking for outliers in numeric columns...\")\n",
        "\n",
        "# Function to detect outliers using IQR method\n",
        "def detect_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)][column]\n",
        "    return outliers.count()\n",
        "\n",
        "# Check outliers in key numeric columns\n",
        "numeric_cols_for_outliers = ['funding_total_usd', 'funding_rounds', 'relationships', 'age_years']\n",
        "for col in numeric_cols_for_outliers:\n",
        "    if col in df_cleaned.columns and df_cleaned[col].notnull().sum() > 0:\n",
        "        outlier_count = detect_outliers(df_cleaned, col)\n",
        "        print(f\"Found {outlier_count} outliers in {col}\")\n",
        "\n",
        "# For funding_total_usd, cap extreme values at 99.9 percentile\n",
        "if 'funding_total_usd' in df_cleaned.columns:\n",
        "    cap_value = df_cleaned['funding_total_usd'].quantile(0.999)\n",
        "    extreme_values = df_cleaned['funding_total_usd'] > cap_value\n",
        "    if extreme_values.sum() > 0:\n",
        "        print(f\"Capping {extreme_values.sum()} extreme funding values at {cap_value}\")\n",
        "        df_cleaned.loc[extreme_values, 'funding_total_usd'] = cap_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Creating cleaning summary...\n",
            "Columns in drop list that existed in original dataframe: 12 out of 12\n",
            "Difference in column count between original and cleaned dataframes: 8\n",
            "New columns added during cleaning: {'has_founding_date', 'has_funding_info', 'has_location_info', 'has_complete_core_info'}\n",
            "Original Rows: 196553\n",
            "Original Columns: 52\n",
            "Cleaned Rows: 196553\n",
            "Cleaned Columns: 44\n",
            "Rows Removed: 0\n",
            "Columns Removed: 8\n",
            "Duplicate Rows Found: 0\n",
            "Columns Dropped: ['parent_id', 'closed_at', 'short_description', 'first_investment_at', 'last_investment_at', 'investment_rounds', 'invested_companies', 'ROI', 'Unnamed: 0.1', 'logo_url', 'logo_width', 'logo_height']\n",
            "Companies with Complete Core Info: 196553\n",
            "Companies with Location Info: 196553\n",
            "Companies with Funding Info: 196553\n",
            "Companies with Founding Date: 91227\n",
            "\n",
            "Verification:\n",
            "Reported columns removed: 8\n",
            "Actual columns dropped: 12\n",
            "Warning: Mismatch between reported columns removed and actual columns dropped!\n",
            "Columns in original but not in cleaned dataframe: {'first_investment_at', 'closed_at', 'invested_companies', 'parent_id', 'ROI', 'Unnamed: 0.1', 'investment_rounds', 'logo_width', 'short_description', 'logo_url', 'last_investment_at', 'logo_height'}\n",
            "Columns correctly dropped (in drop list): ['first_investment_at', 'closed_at', 'invested_companies', 'parent_id', 'ROI', 'Unnamed: 0.1', 'investment_rounds', 'logo_width', 'short_description', 'logo_url', 'last_investment_at', 'logo_height']\n",
            "Expected columns after dropping: 44\n",
            "Actual columns after dropping: 44\n",
            "Difference explained by 4 new columns added during cleaning: {'has_founding_date', 'has_funding_info', 'has_location_info', 'has_complete_core_info'}\n"
          ]
        }
      ],
      "source": [
        "# Create a summary of the cleaning process\n",
        "print(\"\\nCreating cleaning summary...\")\n",
        "\n",
        "# Original shape\n",
        "original_shape = df.shape\n",
        "# Cleaned shape\n",
        "cleaned_shape = df_cleaned.shape\n",
        "\n",
        "# Calculate changes\n",
        "rows_removed = original_shape[0] - cleaned_shape[0]\n",
        "\n",
        "# Get unique columns dropped (remove duplicates)\n",
        "unique_columns_dropped = list(dict.fromkeys(columns_to_drop))\n",
        "\n",
        "# Check which columns from columns_to_drop actually existed in the original dataframe\n",
        "columns_actually_dropped = [col for col in unique_columns_dropped if col in df.columns]\n",
        "print(f\"Columns in drop list that existed in original dataframe: {len(columns_actually_dropped)} out of {len(unique_columns_dropped)}\")\n",
        "\n",
        "# Calculate the actual number of columns removed\n",
        "columns_removed = original_shape[1] - cleaned_shape[1]\n",
        "print(f\"Difference in column count between original and cleaned dataframes: {columns_removed}\")\n",
        "\n",
        "# Check for new columns that might have been added during cleaning\n",
        "original_cols_set = set(df.columns)\n",
        "cleaned_cols_set = set(df_cleaned.columns)\n",
        "new_columns = cleaned_cols_set - original_cols_set\n",
        "if new_columns:\n",
        "    print(f\"New columns added during cleaning: {new_columns}\")\n",
        "\n",
        "# Create a summary dictionary\n",
        "cleaning_summary = {\n",
        "    'Original Rows': original_shape[0],\n",
        "    'Original Columns': original_shape[1],\n",
        "    'Cleaned Rows': cleaned_shape[0],\n",
        "    'Cleaned Columns': cleaned_shape[1],\n",
        "    'Rows Removed': rows_removed,\n",
        "    'Columns Removed': columns_removed,\n",
        "    'Duplicate Rows Found': duplicate_rows,\n",
        "    'Columns Dropped': columns_actually_dropped,  # Use the columns that actually existed\n",
        "    'Companies with Complete Core Info': df_cleaned['has_complete_core_info'].sum(),\n",
        "    'Companies with Location Info': df_cleaned['has_location_info'].sum(),\n",
        "    'Companies with Funding Info': df_cleaned['has_funding_info'].sum(),\n",
        "    'Companies with Founding Date': df_cleaned['has_founding_date'].sum()\n",
        "}\n",
        "\n",
        "# Print summary\n",
        "for key, value in cleaning_summary.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "# Add a verification check to ensure our counts match\n",
        "print(\"\\nVerification:\")\n",
        "print(f\"Reported columns removed: {columns_removed}\")\n",
        "print(f\"Actual columns dropped: {len(columns_actually_dropped)}\")\n",
        "\n",
        "# Detailed analysis if there's a mismatch\n",
        "if columns_removed != len(columns_actually_dropped):\n",
        "    print(\"Warning: Mismatch between reported columns removed and actual columns dropped!\")\n",
        "    \n",
        "    # Find columns that were in the original but not in the cleaned dataframe\n",
        "    missing_cols = original_cols_set - cleaned_cols_set\n",
        "    print(f\"Columns in original but not in cleaned dataframe: {missing_cols}\")\n",
        "    \n",
        "    # Check which of these missing columns were in our drop list\n",
        "    expected_drops = [col for col in missing_cols if col in columns_to_drop]\n",
        "    print(f\"Columns correctly dropped (in drop list): {expected_drops}\")\n",
        "    \n",
        "    # Find columns that were dropped but not in our drop list\n",
        "    unexpected_drops = [col for col in missing_cols if col not in columns_to_drop]\n",
        "    if unexpected_drops:\n",
        "        print(f\"Columns dropped but not in our drop list: {unexpected_drops}\")\n",
        "    \n",
        "    # Find columns that were in columns_to_drop but not actually dropped\n",
        "    not_dropped = [col for col in columns_actually_dropped if col in cleaned_cols_set]\n",
        "    if not_dropped:\n",
        "        print(f\"Columns in drop list but not actually dropped: {not_dropped}\")\n",
        "    \n",
        "    # Find columns that were in columns_to_drop but not in the original dataframe\n",
        "    nonexistent_cols = [col for col in columns_to_drop if col not in df.columns]\n",
        "    if nonexistent_cols:\n",
        "        print(f\"Columns in drop list that didn't exist in original dataframe: {nonexistent_cols}\")\n",
        "    \n",
        "    # Calculate the expected number of columns after dropping\n",
        "    expected_cols_after_drop = original_shape[1] - len(columns_actually_dropped) + len(new_columns)\n",
        "    print(f\"Expected columns after dropping: {expected_cols_after_drop}\")\n",
        "    print(f\"Actual columns after dropping: {cleaned_shape[1]}\")\n",
        "    \n",
        "    # Reconcile the difference\n",
        "    if len(new_columns) > 0:\n",
        "        print(f\"Difference explained by {len(new_columns)} new columns added during cleaning: {new_columns}\")\n",
        "    \n",
        "    if len(not_dropped) > 0:\n",
        "        print(f\"Difference explained by {len(not_dropped)} columns in drop list not actually dropped: {not_dropped}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saving cleaned dataset...\n",
            "Saved cleaned dataset to 'cleaned_companies.csv'\n"
          ]
        }
      ],
      "source": [
        "# Save the cleaned dataset\n",
        "print(\"\\nSaving cleaned dataset...\")\n",
        "df_cleaned.to_csv('cleaned_companies.csv', index=False)\n",
        "print(\"Saved cleaned dataset to 'cleaned_companies.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generating cleaning report...\n",
            "Generated cleaning report: 'cleaning_report.md'\n"
          ]
        }
      ],
      "source": [
        "# Generate cleaning report markdown file\n",
        "print(\"\\nGenerating cleaning report...\")\n",
        "\n",
        "report_content = f\"\"\"# Data Cleaning Report for Crunchbase Companies Dataset\n",
        "\n",
        "## Overview\n",
        "This report documents the data cleaning process applied to the Crunchbase companies dataset for the startup success prediction project.\n",
        "\n",
        "## Dataset Information\n",
        "- **Original Dataset**: {original_shape[0]} rows, {original_shape[1]} columns\n",
        "- **Cleaned Dataset**: {cleaned_shape[0]} rows, {cleaned_shape[1]} columns\n",
        "- **Rows Removed**: {rows_removed}\n",
        "- **Columns Removed**: {columns_removed}\n",
        "\n",
        "## Cleaning Steps Performed\n",
        "\n",
        "### 1. Duplicate Handling\n",
        "- Checked for duplicate rows: {duplicate_rows} found and removed\n",
        "- Checked for duplicate company IDs: {duplicate_ids} found\n",
        "\n",
        "### 2. Column Removal\n",
        "The following columns were removed:\n",
        "- **High Missing Value Columns** (>95% missing): {', '.join(high_missing_cols)}\n",
        "- **Redundant Columns**: {', '.join(redundant_cols)}\n",
        "- **Limited Analytical Value Columns**: {', '.join(limited_value_cols)}\n",
        "\n",
        "### 3. Missing Value Treatment\n",
        "- **Categorical Columns**: Filled with 'unknown' ({', '.join(categorical_cols)})\n",
        "- **Text Columns**: Filled with empty string ({', '.join(text_cols)})\n",
        "- **Numeric Funding Columns**: Filled with 0 ({', '.join(funding_cols)})\n",
        "- **Date Columns**: Kept as NaN to avoid introducing bias ({', '.join(date_cols)})\n",
        "\n",
        "### 4. Text Standardization\n",
        "- Cleaned and standardized text in: {', '.join(text_cols_to_clean)}\n",
        "- Standardized URLs in: {', '.join(url_cols)}\n",
        "\n",
        "### 5. Geographic Data Standardization\n",
        "- Converted country codes to uppercase\n",
        "- Converted state codes to uppercase\n",
        "- Validated latitude/longitude values\n",
        "\n",
        "### 6. Data Type Correction\n",
        "- Converted numeric columns to appropriate numeric types\n",
        "- Converted categorical columns to category type\n",
        "\n",
        "### 7. Data Quality Flags\n",
        "- **Companies with Complete Core Info**: {df_cleaned['has_complete_core_info'].sum()} ({df_cleaned['has_complete_core_info'].mean()*100:.2f}%)\n",
        "- **Companies with Location Info**: {df_cleaned['has_location_info'].sum()} ({df_cleaned['has_location_info'].mean()*100:.2f}%)\n",
        "- **Companies with Funding Info**: {df_cleaned['has_funding_info'].sum()} ({df_cleaned['has_funding_info'].mean()*100:.2f}%)\n",
        "- **Companies with Founding Date**: {df_cleaned['has_founding_date'].sum()} ({df_cleaned['has_founding_date'].mean()*100:.2f}%)\n",
        "\n",
        "### 8. Outlier Handling\n",
        "\"\"\"\n",
        "\n",
        "# Add outlier information to report\n",
        "for col in numeric_cols_for_outliers:\n",
        "    if col in df_cleaned.columns and df_cleaned[col].notnull().sum() > 0:\n",
        "        outlier_count = detect_outliers(df_cleaned, col)\n",
        "        report_content += f\"- **{col}**: {outlier_count} outliers detected\\n\"\n",
        "\n",
        "if 'funding_total_usd' in df_cleaned.columns:\n",
        "    cap_value = df_cleaned['funding_total_usd'].quantile(0.999)\n",
        "    extreme_values = df_cleaned['funding_total_usd'] > cap_value\n",
        "    report_content += f\"- Capped {extreme_values.sum()} extreme funding values at {cap_value:.2f}\\n\"\n",
        "\n",
        "report_content += \"\"\"\n",
        "## Impact on Analysis\n",
        "The cleaning process has:\n",
        "1. Removed redundant and low-value columns to focus the analysis\n",
        "2. Handled missing values appropriately based on column context\n",
        "3. Standardized text and geographic data for consistency\n",
        "4. Added data quality flags to help filter companies for analysis\n",
        "5. Addressed outliers in key numeric columns\n",
        "\n",
        "## Next Steps\n",
        "The cleaned dataset is now ready for feature engineering, where we will:\n",
        "1. Create derived features from existing data\n",
        "2. Extract insights from text fields\n",
        "3. Develop time-based metrics\n",
        "4. Prepare the data for modeling\n",
        "\"\"\"\n",
        "\n",
        "# Write report to file\n",
        "with open('cleaning_report.md', 'w') as f:\n",
        "    f.write(report_content)\n",
        "\n",
        "print(\"Generated cleaning report: 'cleaning_report.md'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final dataset information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 196553 entries, 0 to 196552\n",
            "Data columns (total 44 columns):\n",
            " #   Column                  Non-Null Count   Dtype         \n",
            "---  ------                  --------------   -----         \n",
            " 0   id                      196553 non-null  object        \n",
            " 1   entity_type             196553 non-null  category      \n",
            " 2   entity_id               196553 non-null  Int64         \n",
            " 3   name                    196553 non-null  object        \n",
            " 4   normalized_name         196553 non-null  object        \n",
            " 5   permalink               196553 non-null  object        \n",
            " 6   category_code           196553 non-null  category      \n",
            " 7   status                  196553 non-null  category      \n",
            " 8   founded_at              91227 non-null   datetime64[ns]\n",
            " 9   domain                  196553 non-null  object        \n",
            " 10  homepage_url            196553 non-null  object        \n",
            " 11  twitter_username        196553 non-null  object        \n",
            " 12  description             196553 non-null  object        \n",
            " 13  overview                196553 non-null  object        \n",
            " 14  tag_list                196553 non-null  object        \n",
            " 15  country_code            196553 non-null  category      \n",
            " 16  state_code              196553 non-null  category      \n",
            " 17  city                    196553 non-null  object        \n",
            " 18  region                  196553 non-null  category      \n",
            " 19  first_funding_at        31507 non-null   datetime64[ns]\n",
            " 20  last_funding_at         31507 non-null   datetime64[ns]\n",
            " 21  funding_rounds          196553 non-null  Int64         \n",
            " 22  funding_total_usd       196553 non-null  float64       \n",
            " 23  first_milestone_at      91699 non-null   datetime64[ns]\n",
            " 24  last_milestone_at       91699 non-null   datetime64[ns]\n",
            " 25  milestones              196553 non-null  Int64         \n",
            " 26  relationships           196553 non-null  Int64         \n",
            " 27  created_by              155533 non-null  object        \n",
            " 28  created_at              196553 non-null  datetime64[ns]\n",
            " 29  updated_at              196553 non-null  datetime64[ns]\n",
            " 30  lat                     83852 non-null   float64       \n",
            " 31  lng                     83852 non-null   float64       \n",
            " 32  age_years               91227 non-null   float64       \n",
            " 33  success_status          196553 non-null  Int64         \n",
            " 34  success_funding         196553 non-null  Int64         \n",
            " 35  success_roi             196553 non-null  Int64         \n",
            " 36  success_age             196553 non-null  Int64         \n",
            " 37  success_score           196553 non-null  Int64         \n",
            " 38  success_binary          196553 non-null  Int64         \n",
            " 39  success_class           196553 non-null  category      \n",
            " 40  has_complete_core_info  196553 non-null  bool          \n",
            " 41  has_location_info       196553 non-null  bool          \n",
            " 42  has_funding_info        196553 non-null  bool          \n",
            " 43  has_founding_date       196553 non-null  bool          \n",
            "dtypes: Int64(10), bool(4), category(7), datetime64[ns](7), float64(4), object(12)\n",
            "memory usage: 54.0+ MB\n",
            "None\n",
            "\n",
            "Sample of cleaned data:\n",
            "        id entity_type  entity_id                name     normalized_name                    permalink    category_code     status founded_at                domain                 homepage_url twitter_username                  description                                           overview                                           tag_list country_code state_code         city       region first_funding_at last_funding_at  funding_rounds  funding_total_usd first_milestone_at last_milestone_at  milestones  relationships        created_by          created_at          updated_at   lat     lng  age_years  success_status  success_funding  success_roi  success_age  success_score  success_binary success_class  has_complete_core_info  has_location_info  has_funding_info  has_founding_date\n",
            "0      c:1     Company          1            Wetpaint            wetpaint            /company/wetpaint              web  operating 2005-10-17      wetpaint-inc.com      http://wetpaint-inc.com  BachelrWetpaint  Technology Platform Company  Wetpaint is a technology platform company that...  wiki, seattle, elowitz, media-industry, media-...          USA         WA      Seattle      Seattle       2005-10-01      2008-05-19               3        39750000.00         2010-09-05        2013-09-18           5             17  initial-importer 2007-05-25 06:51:27 2013-04-13 03:29:00 47.61 -122.33      17.21               0                1            1            0              4               1  high_success                    True               True              True               True\n",
            "1     c:10     Company         10             Flektor             flektor             /company/flektor      games_video   acquired        NaT           flektor.com       http://www.flektor.com                                                Flektor is a rich-media mash-up platform that ...                              flektor, photo, video          USA         CA  Culver City  Los Angeles              NaT             NaT               0               0.00                NaT               NaT           0              6  initial-importer 2007-05-31 21:11:51 2008-05-23 23:23:14 34.02 -118.40        NaN               1                0            0            0              3               1  high_success                    True               True              True              False\n",
            "2    c:100     Company        100               There               there               /company/there      games_video   acquired        NaT             there.com         http://www.there.com                                                There.com is an online virtual world where any...                         virtualworld, there, teens          USA         CA    San Mateo       SF Bay              NaT             NaT               0               0.00         2003-02-01        2011-09-23           4             12  initial-importer 2007-08-06 23:52:45 2013-11-04 02:09:48 37.56 -122.33        NaN               1                0            0            0              3               1  high_success                    True               True              True              False\n",
            "3  c:10000     Company      10000             MYWEBBO             mywebbo             /company/mywebbo  network_hosting  operating 2008-07-26           mywebbo.com       http://www.mywebbo.com                                                BRAND NEW ONLINE SOCIAL NETWORKING WEBSITE,FOR...  social-network, new, website, web, friends, ch...      UNKNOWN    UNKNOWN      unknown      unknown              NaT             NaT               0               0.00                NaT               NaT           0              0               NaN 2008-08-24 16:51:57 2008-09-06 14:19:18   NaN     NaN      14.43               0                0            0            0              0               0  unsuccessful                    True               True              True               True\n",
            "4  c:10001     Company      10001  THE Movie Streamer  the movie streamer  /company/the-movie-streamer      games_video  operating 2008-07-26  themoviestreamer.com  http://themoviestreamer.com                                                This company shows free movies online on their...  watch, full-length, moives, online, for, free,...      UNKNOWN    UNKNOWN      unknown      unknown              NaT             NaT               0               0.00                NaT               NaT           0              0               NaN 2008-08-24 17:10:34 2008-09-06 14:19:18   NaN     NaN      14.43               0                0            0            0              0               0  unsuccessful                    True               True              True               True\n",
            "\n",
            "Success metrics distribution:\n",
            "Success binary distribution:\n",
            "success_binary\n",
            "0    182702\n",
            "1     13851\n",
            "Name: count, dtype: Int64\n",
            "Success class distribution:\n",
            "success_class\n",
            "unsuccessful        162823\n",
            "low_success          19879\n",
            "high_success         11409\n",
            "moderate_success      2442\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Display final dataset information\n",
        "print(\"\\nFinal dataset information:\")\n",
        "print(df_cleaned.info())\n",
        "\n",
        "# Display sample of cleaned data\n",
        "print(\"\\nSample of cleaned data:\")\n",
        "print(df_cleaned.head())\n",
        "\n",
        "# Display success metrics distribution\n",
        "print(\"\\nSuccess metrics distribution:\")\n",
        "if 'success_binary' in df_cleaned.columns:\n",
        "    print(f\"Success binary distribution:\\n{df_cleaned['success_binary'].value_counts(dropna=False)}\")\n",
        "if 'success_class' in df_cleaned.columns:\n",
        "    print(f\"Success class distribution:\\n{df_cleaned['success_class'].value_counts(dropna=False)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "erdos_spring_2025",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
